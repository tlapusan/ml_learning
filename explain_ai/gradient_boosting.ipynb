{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://explained.ai/gradient-boosting/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tuning GBM hyperparameters is required to get a good model, unlike Random Forest. \n",
    "- To tune GBM hyperparameters, we need to have a deep understanding of how GBM works.\n",
    "- article's goals\n",
    "    - explain the intuition behind GBM\n",
    "    - explain the mathematics as simple as possible\n",
    "    - answer the question why GBM is performing \"gradient descent\"\n",
    "- GBM is a <b>composite model</b> which combine the efforts of multiple weak models to create in the end one strong model. Each additinal weak model should reduce the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting : distance to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to additive modeling\n",
    "- additive modeling is the fundation of boosting\n",
    "- additive modeling = adding multiple simple terms together to create a more complex expression.\n",
    "- expression/function in ML means a model which tries to map as good as possible the  feature values to a scalar vector, y.\n",
    "- decomposing a complex function into simple subfuntion is commonly known as [devide and conquer](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm) programming algorithm.\n",
    "\n",
    "### Introduction to boosted regression\n",
    "- boostring is a strategy that combines multiple single models into a single composite model.\n",
    "- the idea is that, as we add more and more weak modes, the general model will become stronger and stronger predictor.\n",
    "- in boosting, simple models are called weak models or weak learners.\n",
    "- in the context of regression, the model will make numerical predictions. Given a single feature vector x and scalar target value y for a single observation, the composite model will make its predictions as the sum of all predictions made by all its submodels.\n",
    "- submodels in boosting can be any kind of ML model, starting from k-nearest-neighbors or regression trees. In practice, all boostring algorighms are using trees.\n",
    "- even if additive models could build its submodels individually and in parallel, boosting constructs its submodel in a sequential order, with the goal of each submodel to improve the overall model performance.\n",
    "- number of submodules is chosen by a hyperparamenter, M. Allowing M to grow arbitrary, can cause the risk of overfitting.\n",
    "\n",
    "### Intuition behind gradient boosting\n",
    "- to create a boosting model, first let's create a crappy model, f0(x), that predicts an initial approximation of y, given feature vector x.\n",
    "- then, let's gradually nudge the overall Fm(x) toward the known y value by adding one or more tweaks, Delta_m(x).\n",
    "\n",
    "<img src='https://explained.ai/gradient-boosting/images/latex-C038479244EA27FF65B7DD5CD6F6574A.svg'>\n",
    "- or using a recursive expression :\n",
    "<img src='https://explained.ai/gradient-boosting/images/latex-6E99DC1985D974714E94D70CDE598C18.svg'>\n",
    "- to imagine how a gradient boosting tries to hit the final prediction, think how a golf player proceed\n",
    "<img src='https://explained.ai/gradient-boosting/images/golf-dir-vector.png'>\n",
    "\n",
    "- the difference between predicted y value and original y value is known as residual vector. For gradient boosting it's helpful to think at it as the vector pointing to the original y value.\n",
    "- the next submodel will be trained on the residual vector from the previous submodel\n",
    "- GMB also supports learning rate, what speed up or down the learning process, which helps to reduce the likelihood of overfitting.\n",
    "- \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
