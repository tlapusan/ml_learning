{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are personal notes from https://explained.ai/gradient-boosting/index.html. <br>\n",
    "The goal of this notebook is to make a resume with the most important information from the tutorial !\n",
    "This will help me to remember faster and easier the key information, without reading again the whole article !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tuning GBM hyperparameters is required to get a good model, unlike Random Forest. \n",
    "- To tune GBM hyperparameters, we need to have a deep understanding of how GBM works.\n",
    "- article's goals\n",
    "    - explain the intuition behind GBM\n",
    "    - explain the mathematics as simple as possible\n",
    "    - answer the question why GBM is performing \"gradient descent\"\n",
    "- GBM is a <b>composite model</b> which combine the efforts of multiple weak models to create in the end one strong model. Each additinal weak model should reduce the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting : distance to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to additive modeling\n",
    "- additive modeling is the fundation of boosting\n",
    "- additive modeling = adding multiple simple terms together to create a more complex expression.\n",
    "- expression/function in ML means a model which tries to map as good as possible the  feature values to a scalar vector, y.\n",
    "- decomposing a complex function into simple subfunction is commonly known as [devide and conquer](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm) programming algorithm.\n",
    "\n",
    "### Introduction to boosted regression\n",
    "- boostring is a strategy that combines multiple single models into a single composite model.\n",
    "- the idea is that, as we add more and more weak modes, the general model will become stronger and stronger predictor.\n",
    "- in boosting, simple models are called weak models or weak learners.\n",
    "- in the context of regression, the model will make numerical predictions. Given a single feature vector x and scalar target value y for a single observation, the composite model will make its predictions as the sum of all predictions made by all its weak models.\n",
    "- weak models in boosting can be any kind of ML model, starting from k-nearest-neighbors or regression trees. In practice, all boostring algorighms are using trees.\n",
    "- even if additive models could build its weak models individually and in parallel, boosting constructs its weak models in a sequential order, with the goal of each weak model to improve the overall model performance.\n",
    "- number of weak modules is chosen by a hyperparamenter, M. Allowing M to grow arbitrary, can cause the risk of overfitting.\n",
    "\n",
    "### Intuition behind gradient boosting\n",
    "- to create a boosting model, first let's create a crappy model, f0(x), that predicts an initial approximation of y, given feature vector x.\n",
    "- then, let's gradually nudge the overall Fm(x) toward the known y value by adding one or more tweaks, Delta_m(x).\n",
    "\n",
    "<img src='https://explained.ai/gradient-boosting/images/latex-C038479244EA27FF65B7DD5CD6F6574A.svg'>\n",
    "- or using a recursive expression :\n",
    "<img src='https://explained.ai/gradient-boosting/images/latex-6E99DC1985D974714E94D70CDE598C18.svg'>\n",
    "- to imagine how a gradient boosting tries to hit the final prediction, think how a golf player proceed\n",
    "<img src='https://explained.ai/gradient-boosting/images/golf-dir-vector.png'>\n",
    "\n",
    "- the difference between predicted y value and original y value is known as residual vector. For gradient boosting it's helpful to think at it as the vector pointing to the original y value.\n",
    "- the next submodel will be trained on the residual vector from the previous submodel\n",
    "- GMB also supports learning rate, what speed up or down the learning process, which helps to reduce the likelihood of overfitting.\n",
    "- Leaning rate is also known as shrinkage (contractare)\n",
    "    - shrinkage reduce the influence of each individual tree, and leaves space for future trees to improve the predictions\n",
    "    - Friedman recomends a low learning rate (0.1) and more stages, M\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GBM, for regression, can be trained in two ways : \n",
    "- distance to target, where the weak models are trained on direction and magnitude of residual values\n",
    "    - using the residual vector, will have the effect to minimize MSE loss function.\n",
    "- direction to target, where the weak models are trained only on direction, namely the sign of residual values (-1, 0, 1)\n",
    "    - using the sign of residual vector, will have the effect of minimize another loss function, MAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM - direction to target\n",
    "- distance-to-target, where both the direction and magnitude are taken in consideration, has the drawback to be biased by outliers. Why ? Because the leaf predictions are based on averages, and outliers influence the mean value.\n",
    "- when we have a dataset and it contains many outliers in y values, it's recommended to train GBM for regression using direction to target.\n",
    "- if our loss function is MAE, this means our leaf predictions will be the median, instead of the mean.\n",
    "- the initial model, f0, will predict the median on Y values.\n",
    "- the model is trained using direction, but in the end the leaf with predict the median of residual values ? why, because if the leaf will predict the median of direction, the composite model will converge very, very slow. There is the need to weight somehow each leaf... by predicting the median we can think as predicting the direction and scaling/weighting by median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
